---
title: 论文速读
mathjax: true
typora-root-url: ..
date: 2021-09-16 19:30:20
categories:
tags:
---

CAT-SAC: Soft Actor-Critic with Curiosity-Aware Entropy Temperature, ICRL 2021

Stabilizing Deep Q-Learning with ConvNets and Vision Transformers under Data Augmentation, arXiv:2107.00644 [cs]

Self-Supervised Policy Adaptation during Deployment, ICLR 2021

<!--more-->

# CAT-SAC: Soft Actor-Critic with Curiosity-Aware Entropy Temperature

在调节 SAC 的温度参数 $\alpha$ 时，使用好奇心机制来动态调整 $\alpha$ ，起到平衡探索与利用的作用，而不只是单纯的一个策略中所有状态都使用同一个温度参数。 

首先，对于原始 SAC 设置的一个固定的 target entropy $\tilde{\mathcal{H}}$ 调整为：
$$
h(s)=\tilde{\mathcal{H}}+\frac{c(s)-\mu}{\sigma}
$$
其中 $c(s)$ 表示状态 $s$ 的好奇程度，$\mu, \sigma$ 分别为 $c(s)$ 的移动平均和移动标准差。此时从期望角度来看，target entropy 依然保持不变：$\mathbb{E}_{s}[h(s)]=\tilde{\mathcal{H}}$。

接下来，对于温度参数 $\alpha$ ，将它变为以 $\delta$ 为参数，关于状态 $s$ 的函数： $\alpha_\delta(s_t)$ ，这样对于不同的状态，可以对应不同的温度参数，而不仅仅像以前那样对应相同的温度参数。此时，温度参数的更新公式为：
$$
\delta^{*}=\underset{\delta}{\operatorname{argmin}} \mathbb{E}_{s_{t} \sim \mathcal{D}, a_{t} \sim \pi_{\phi}}\left[-\alpha_{\delta}\left(s_{t}\right)\left(\log \pi_{\phi}\left(a_{t} | s_{t}\right)+h\left(s_{t}\right)\right)\right]
$$
作者将 $\alpha_\delta(s_t)$ 通过神经网络，构造为 $\alpha_{\delta}\left(s_{t}\right)=g_{\delta}\left(c\left(s_{t}\right)\right)$ 与状态的好奇心程度有关的函数，使得拥有相同好奇心的状态，拥有相同的温度参数，防止在图像输入的情况下， $\alpha_\delta(s_t)$ 太过于剧烈。

除此之外，作者还使用了对比学习方法来学习基于 RND 的好奇心，称为 X-RND，来解决在未见过的状态特征与见过的状态特征比较接近时，好奇心较小的问题。



# Stabilizing Deep Q-Learning with ConvNets and Vision Transformers under Data Augmentation

在强化学习中使用图像增强技术可以增加策略的泛化性，但有时却会降低样本利用率，训练不稳定，甚至会无法收敛，主要问题体现在：

1. 图像增强太过于随机，使得在计算 target $Q$ 的时候的方差太大
2. 图像增强过后的图像 state 与原始图像 state 差距太大，但增强的图像数量太多，主导了梯度的方向，并不能因此学到一个更好的 $Q$

解决方法：

![](/images/2021-09-16-论文速读/image-20210916194131730.png)

计算 target $Q$ 时，不用增强后的图像 state。而在计算 online $Q$ 时，共同使用原始图像和增强后的图像。
$$
\begin{aligned}
\mathcal{L}_{Q}^{\mathrm{SVEA}}(\theta, \psi) & \triangleq {\color{blue}\alpha \mathcal{L}_{Q}\left(\mathrm{~s}_{t}, q_{t}^{\mathrm{tgt}} ; \theta, \psi\right)}+ {\color{red}\beta \mathcal{L}_{Q}\left(\mathrm{~s}_{t}^{\mathrm{aug}}, q_{t}^{\mathrm{tgt}} ; \theta, \psi\right) }\\
&=\mathbb{E}_{\mathbf{s}_{t}, \mathbf{a}_{t}, \mathbf{s}_{t+1} \sim \mathcal{B}}\left[\alpha\left\|Q_{\theta}\left(f_{\theta}\left(\mathbf{s}_{t}\right), \mathbf{a}_{t}\right)-q_{t}^{\mathrm{tgt}}\right\|_{2}^{2}+\beta\left\|Q_{\theta}\left(f_{\theta}\left(\mathrm{s}_{t}^{\text {aug }}\right), \mathbf{a}_{t}\right)-q_{t}^{\mathrm{tg}}\right\|_{2}^{2}\right]
\end{aligned}
$$
其中 $q_t^{\mathrm{tgt}}$ 直接使用为增强后的图像 state 来进行计算，蓝色部分表示更新为增强的，红色则是增强后的，$\alpha$ 和 $\beta$ 为调节两者的系数。



# Self-Supervised Policy Adaptation during Deployment

由于强化学习训练的环境与现实部署的环境有差别，为了消除不同环境的差异，增加策略的泛化性，可以在部署推断的阶段继续训练。但是在现实环境中由于可能没有奖励信号，再使用强化学习训练就会变得不可行。所以论文中使用了额外的一个自监督方式来继续训练。

![](/images/2021-09-16-论文速读/image-20210917205909818.png)

可以看出，左边的训练阶段，除了常规的强化学习训练，还有一层自监督学习，并且两者都共享一个表征模型。而在右边的部署阶段，停止强化学习的训练，但继续训练自监督学习来持续更新表征模块。

论文中主要使用了 Inverse Dynamic Prediction 和 Rotation Prediction 来作为自监督学习的方式。

Inverse Dynamic Prediction 即是通过前后两帧的状态来预测执行的动作：
$$
L\left(\theta_{s}, \theta_{e}\right)=\ell\left(\mathbf{a}_{t}, \pi_{s}\left(\pi_{e}\left(\mathbf{s}_{t}\right), \pi_{e}\left(\mathbf{s}_{t+1}\right)\right)\right)
$$
而 Rotation Prediction 则将原始图像随即旋转 0、90、180、270 度，并用一个四分类网络判断旋转后的图像到底旋转了多少度。
